# AI-Powered Resume Analyzer

## ğŸ™Œ **How to run on Windows ?**

- Create local virtual env
- Install Rust (https://rustup.rs/)
- Add .cargo/bin to PATH
- Import requirements
- Run: streamlit run app.py
- Visit: http://localhost:8501/

**AI-Powered Resume Analyzer**, a cutting-edge application designed to mimic the expertise of an HR professional! This tool leverages the power of **Google Generative AI** to analyze resumes, evaluate job compatibility, and offer actionable insights for career enhancement.

---

## ğŸ“‹ **Project Overview**

The **AI-Powered Resume Analyzer** serves as a virtual HR assistant, providing:

- Detailed resume evaluation, including strengths and weaknesses.
- Suggestions for skill improvement and recommended courses.
- Job-specific resume analysis to measure compatibility and alignment with job descriptions.

Whether youâ€™re a job seeker or a recruiter, this tool simplifies resume assessment and improvement.

---

## ğŸ”‘ **Features**

### 1ï¸âƒ£ **General Resume Analysis**

- Summarizes the resume in one line.
- Highlights existing skill sets.
- Identifies skill gaps and suggests improvements.
- Recommends popular courses to enhance the resume.
- Provides a thorough evaluation of strengths and weaknesses.

### 2ï¸âƒ£ **Resume Matching with Job Description**

- Analyzes resume compatibility with a specific job description.
- Provides a match score in percentage.
- Highlights missing skills and areas needing improvement.
- Suggests whether the resume is ready for the job or requires further enhancements.

---

## ğŸ› ï¸ **Tech Stack**

| **Component**          | **Technology**                                                           |
| ---------------------- | ------------------------------------------------------------------------ |
| **Frontend**           | [Streamlit](https://streamlit.io/)                                       |
| **Backend**            | Python                                                                   |
| **AI Model**           | [Google Generative AI (Gemini)](https://developers.generativeai.google/) |
| **PDF Parsing**        | `pdfplumber`                                                             |
| **OCR Fallback**       | `pytesseract`                                                            |
| **Environment Config** | `.env` for API key security                                              |

---

## ğŸ“Š **How It Works**

1. **Resume Parsing**

   - Extracts text from PDF files using `pdfplumber` or OCR as a fallback.

2. **AI Analysis**

   - Utilizes Google Generative AI to summarize and analyze resume content.
   - Matches skills with job descriptions for compatibility scoring.

3. **Insightful Feedback**
   - Provides actionable suggestions for skill enhancement, including course recommendations.
   - Highlights strengths and weaknesses to refine resumes for better opportunities.

---

![image](https://github.com/user-attachments/assets/418e54ef-82d0-474b-a6bc-9a30d72f27f5)

## ğŸ™Œ **Contributing**

Welcome contributions to make this tool better!

1. **Fork** the repository.
2. **Create a new branch** for your feature or bug fix.
3. **Submit a pull request** with detailed information about your changes.

Ensure that you have access to the API key for Google Gemini and that you've set up authentication correctly using:
Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)

# ğŸ§  Generative AI Resume Processor

This project processes resumes (CVs) using Gemini and OpenAI models, extracts key candidate information, and stores results in PostgreSQL. It's designed using a modular, scalable structure for easy maintenance and experimentation.

---

## ğŸ“‚ Project Structure

```bash
generative_ai_project/
â”‚
â”œâ”€â”€ config/                  # Configuration (YAML-based)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_config.yaml    # LLM keys and model names
â”‚   â”œâ”€â”€ prompt_templates.yaml# Few-shot and system prompts
â”‚   â””â”€â”€ logging_config.yaml  # Logging levels and formats
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ llm/                 # Language model clients
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py          # Abstract LLM base class
â”‚   â”‚   â”œâ”€â”€ gpt_client.py    # OpenAI client implementation
â”‚   â”‚   â””â”€â”€ gemini_client.py # Gemini client (Google)
â”‚   â”‚
â”‚   â”œâ”€â”€ prompt_engineering/  # Prompt formatting logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ templates.py
â”‚   â”‚   â”œâ”€â”€ few_shot.py
â”‚   â”‚   â””â”€â”€ chainer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/               # Common tools
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py
â”‚   â”‚   â”œâ”€â”€ token_counter.py
â”‚   â”‚   â”œâ”€â”€ cache.py
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”‚
â”‚   â””â”€â”€ handlers/            # Resume analysis and DB writer
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ error_handler.py
â”‚
â”œâ”€â”€ data/                    # Input/Output
â”‚   â”œâ”€â”€ cache/               # Intermediate cache
â”‚   â”œâ”€â”€ prompts/             # Prompt templates
â”‚   â”œâ”€â”€ outputs/             # Generated SQL dumps
â”‚   â””â”€â”€ embeddings/          # Saved vectors (future use)
â”‚
â”œâ”€â”€ examples/                # Use case samples
â”‚   â”œâ”€â”€ basic_completion.py
â”‚   â”œâ”€â”€ chat_session.py
â”‚   â””â”€â”€ chain_prompts.py
â”‚
â”œâ”€â”€ notebooks/               # Exploration notebooks
â”‚   â”œâ”€â”€ prompt_testing.ipynb
â”‚   â”œâ”€â”€ response_analysis.ipynb
â”‚   â””â”€â”€ model_experimentation.ipynb
â”‚
â”œâ”€â”€ requirements.txt         # Pip dependencies
â”œâ”€â”€ setup.py                 # Optional package installer
â”œâ”€â”€ README.md                # This file
â””â”€â”€ Dockerfile               # Containerization setup

```

âš™ï¸ Key Components

- config/: Stores model settings, prompt formats, and logging setup.

- src/llm/: Implements language model wrappers for Gemini and OpenAI.

- src/prompt_engineering/: Centralizes reusable prompt strategies.

- src/handlers/: Manages resume parsing, job execution, and DB insertion.

- data/: Storage for raw text, structured outputs, and prompt variations.

- notebooks/: For testing, tuning, and inspecting model behavior.

ğŸš€ Getting Started

- Clone this repo

- Run pip install -r requirements.txt

- Add your .env file with:

- OPENAI_API_KEY=your_key
- GOOGLE_API_KEY=your_key

- Add CVs to S3

- Run main.py or launch the Streamlit UI
